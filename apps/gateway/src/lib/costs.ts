import { Decimal } from "decimal.js";
import { encode, encodeChat } from "gpt-tokenizer";

import { logger } from "@llmgateway/logger";
import {
	type Model,
	type ModelDefinition,
	models,
	type ToolCall,
} from "@llmgateway/models";

// Define ChatMessage type to match what gpt-tokenizer expects
interface ChatMessage {
	role: "user" | "system" | "assistant" | undefined;
	content: string;
	name?: string;
}

const DEFAULT_TOKENIZER_MODEL = "gpt-4";

/**
 * Calculate costs based on model, provider, and token counts
 * If promptTokens or completionTokens are not available, it will try to calculate them
 * from the fullOutput parameter if provided
 */
export function calculateCosts(
	model: Model,
	provider: string,
	promptTokens: number | null,
	completionTokens: number | null,
	cachedTokens: number | null = null,
	fullOutput?: {
		messages?: ChatMessage[];
		prompt?: string;
		completion?: string;
		toolResults?: ToolCall[];
	},
	reasoningTokens: number | null = null,
) {
	// Find the model info - try both base model name and provider model name
	let modelInfo = models.find((m) => m.id === model) as ModelDefinition;

	if (!modelInfo) {
		modelInfo = models.find((m) =>
			m.providers.some((p) => p.modelName === model),
		) as ModelDefinition;
	}

	if (!modelInfo) {
		return {
			inputCost: null,
			outputCost: null,
			cachedInputCost: null,
			requestCost: null,
			totalCost: null,
			promptTokens,
			completionTokens,
			cachedTokens,
			estimatedCost: false,
			discount: undefined,
		};
	}

	// If token counts are not provided, try to calculate them from fullOutput
	let calculatedPromptTokens = promptTokens;
	let calculatedCompletionTokens = completionTokens;
	// Track if we're using estimated tokens
	let isEstimated = false;

	if ((!promptTokens || !completionTokens) && fullOutput) {
		// We're going to estimate at least some of the tokens
		isEstimated = true;
		// Calculate prompt tokens
		if (!promptTokens && fullOutput) {
			if (fullOutput.messages) {
				// For chat messages
				try {
					calculatedPromptTokens = encodeChat(
						fullOutput.messages,
						DEFAULT_TOKENIZER_MODEL,
					).length;
				} catch (error) {
					// If encoding fails, leave as null
					logger.error(`Failed to encode chat messages in costs: ${error}`);
				}
			} else if (fullOutput.prompt) {
				// For text prompt
				try {
					calculatedPromptTokens = encode(
						JSON.stringify(fullOutput.prompt),
					).length;
				} catch (error) {
					// If encoding fails, leave as null
					logger.error(`Failed to encode prompt text: ${error}`);
				}
			}
		}

		// Calculate completion tokens
		if (!completionTokens && fullOutput) {
			let completionText = "";

			// Include main completion content
			if (fullOutput.completion) {
				completionText += fullOutput.completion;
			}

			// Include tool results if available
			if (fullOutput.toolResults && Array.isArray(fullOutput.toolResults)) {
				for (const toolResult of fullOutput.toolResults) {
					if (toolResult.function?.name) {
						completionText += toolResult.function.name;
					}
					if (toolResult.function?.arguments) {
						completionText += JSON.stringify(toolResult.function.arguments);
					}
				}
			}

			if (completionText) {
				try {
					calculatedCompletionTokens = encode(completionText).length;
				} catch (error) {
					// If encoding fails, leave as null
					logger.error(`Failed to encode completion text: ${error}`);
				}
			}
		}
	}

	// If we don't have prompt tokens, we can't calculate any costs
	if (!calculatedPromptTokens) {
		return {
			inputCost: null,
			outputCost: null,
			cachedInputCost: null,
			requestCost: null,
			totalCost: null,
			promptTokens: calculatedPromptTokens,
			completionTokens: calculatedCompletionTokens,
			cachedTokens,
			estimatedCost: isEstimated,
			discount: undefined,
		};
	}

	// Set completion tokens to 0 if not available (but still calculate input costs)
	if (!calculatedCompletionTokens) {
		calculatedCompletionTokens = 0;
	}

	// Find the provider-specific pricing
	const providerInfo = modelInfo.providers.find(
		(p) => p.providerId === provider,
	);

	if (!providerInfo) {
		return {
			inputCost: null,
			outputCost: null,
			cachedInputCost: null,
			requestCost: null,
			totalCost: null,
			promptTokens: calculatedPromptTokens,
			completionTokens: calculatedCompletionTokens,
			cachedTokens,
			estimatedCost: isEstimated,
			discount: undefined,
		};
	}

	const inputPrice = new Decimal(providerInfo.inputPrice || 0);
	const outputPrice = new Decimal(providerInfo.outputPrice || 0);
	const cachedInputPrice = new Decimal(
		providerInfo.cachedInputPrice ?? (providerInfo.inputPrice || 0),
	);
	const requestPrice = new Decimal(providerInfo.requestPrice || 0);
	const discount = new Decimal(providerInfo.discount || 0);
	const discountMultiplier = new Decimal(1).minus(discount);

	// Calculate input cost accounting for cached tokens
	// For Anthropic: calculatedPromptTokens includes all tokens, but we need to subtract cached tokens
	// that get charged at the discounted rate
	// For other providers (like OpenAI), prompt_tokens includes cached tokens, so we subtract them too
	const uncachedPromptTokens = cachedTokens
		? calculatedPromptTokens - cachedTokens
		: calculatedPromptTokens;
	const inputCost = new Decimal(uncachedPromptTokens)
		.times(inputPrice)
		.times(discountMultiplier)
		.toNumber();
	// For Google models, reasoning tokens are billed at the output token rate
	const totalOutputTokens = calculatedCompletionTokens + (reasoningTokens || 0);
	const outputCost = new Decimal(totalOutputTokens)
		.times(outputPrice)
		.times(discountMultiplier)
		.toNumber();
	const cachedInputCost = cachedTokens
		? new Decimal(cachedTokens)
				.times(cachedInputPrice)
				.times(discountMultiplier)
				.toNumber()
		: 0;
	const requestCost = requestPrice.times(discountMultiplier).toNumber();
	const totalCost = new Decimal(inputCost)
		.plus(outputCost)
		.plus(cachedInputCost)
		.plus(requestCost)
		.toNumber();

	return {
		inputCost,
		outputCost,
		cachedInputCost,
		requestCost,
		totalCost,
		promptTokens: calculatedPromptTokens,
		completionTokens: calculatedCompletionTokens,
		cachedTokens,
		estimatedCost: isEstimated,
		discount: !discount.isZero() ? discount.toNumber() : undefined,
	};
}
